from dostoevsky.tokenization import RegexTokenizer
from dostoevsky.models import FastTextSocialNetworkModel

tokenizer = RegexTokenizer()
model = FastTextSocialNetworkModel(tokenizer=tokenizer)

messages = [
    '–ö–∞–∫ –ø–æ–ø–∞–ª–∏, —Ö–æ—Ç–µ–ª–∏ —Ç–æ–Ω–∞ –∫—É–ø–∏—Ç—å –∞ –∫—É–ø–∏–ª–∏ –≤–æ—Ç —ç—Ç–æ –≤–æ—Ç –≤—Å–µ:-) –¢–µ–ø–µ—Ä—å —Ä–∞–∑–æ–±—Ä–∞–ª–∏—Å—å —á—Ç–æ –º—ã –ª–æ—à–∞—Ä—ã, –Ω–æ —Å–∏–¥–∏–º —Ö–æ–ª–¥–∏–º –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π:-)',
    '–î–∞, –≤—Å–µ–º —É–¥–∞—á–∏! –ü—É—Å—Ç—å –ø–æ–±–µ–¥—è—Ç —Å–∏–ª—å–Ω–µ–π—à–∏–µ —Å–∞–º—ã–µ –≤–µ–∑—É—á–∏–µ)',
    '–ü—Ä–∏–≤–µ—Ç. –ñ–µ–ª–∞—é —É–¥–∞—á–∏.',
    '–í—Å–µ–º –ü—Ä–∏–≤–µ—Çüëã',
    '–°–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞',
    '–°–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞',
    '–Ø —Å—á–∞—Å—Ç–ª–∏–≤ –ø—Ä–æ–≤–æ–¥–∏—Ç—å —Å —Ç–æ–±–æ—é –≤—Ä–µ–º—è',
    '–ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è —ç—Ç–∞ –º—É–∑—ã–∫–∞–ª—å–Ω–∞—è –∫–æ–º–ø–æ–∑–∏—Ü–∏—è',
    '–í –±–æ–ª—å–Ω–∏—Ü–µ –±—ã–ª–∞ —É–∂–∞—Å–Ω–∞—è –æ—á–µ—Ä–µ–¥—å',
    '–°–æ—Å–µ–¥ —Å –≤–µ—Ä—Ö–Ω–µ–≥–æ —ç—Ç–∞–∂–∞ –º–µ—à–∞–µ—Ç —Å–ø–∞—Ç—å',
    '–ú–∞–ª–µ–Ω—å–∫–∞—è –¥–µ–≤–æ—á–∫–∞ –ø–æ—Ç–µ—Ä—è–ª–∞—Å—å –≤ —Ç–æ—Ä–≥–æ–≤–æ–º —Ü–µ–Ω—Ç—Ä–µ',
]

results = model.predict(messages, k=2)
for message, sentiment in zip(messages, results):
    print(message, '=', sentiment)
